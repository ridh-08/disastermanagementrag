{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyNH4E91ZcY+t6+z8mFfQvry",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "49e1010f1b6346e6a1964bf23cfaa365": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "Activity 1 – Identify Common Policies",
              "Activity 2 – Evaluate Region-Specific vs. Generalizable Policies",
              "Activity 3 – Assess AI’s Strengths & Limitations",
              "Activity 4 – Score & Rank Policies"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Select Prompt:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_cc8cb2aad908454481e787ccc5d0533e",
            "style": "IPY_MODEL_c4518718d51c42b891c784c1ec6a278d"
          }
        },
        "cc8cb2aad908454481e787ccc5d0533e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "95%"
          }
        },
        "c4518718d51c42b891c784c1ec6a278d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ridh-08/disastermanagementrag/blob/main/promptengineeringrag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlgZLUtUB1m_",
        "outputId": "ef4a9967-dd9c-4cd1-930a-29a4044ce821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (7.7.1)\n",
            "Requirement already satisfied: fpdf in /usr/local/lib/python3.11/dist-packages (1.7.2)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.13.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (1.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets) (6.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (75.2.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets) (6.5.7)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (5.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.6)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (23.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets) (0.2.13)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.0->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (4.3.7)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.2.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.5.1)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.23.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.1->jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets) (1.17.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (21.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (0.24.0)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.16.0)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (2.22)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "# STEP 1: Install packages\n",
        "!pip install python-docx scikit-learn requests ipywidgets fpdf\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Upload documents\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "U-k3Rcc_4yf6",
        "outputId": "276a203f-7f5f-4483-d596-1ce0f1643669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c633ddd0-f148-4ba4-92ce-eb5275cf5f94\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c633ddd0-f148-4ba4-92ce-eb5275cf5f94\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Policy Recommendations for Wildfires.docx to Policy Recommendations for Wildfires (1).docx\n",
            "Saving Earthquake Final Policies.docx to Earthquake Final Policies (2).docx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: Imports and Setup\n",
        "import requests, json\n",
        "from docx import Document\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from fpdf import FPDF\n",
        "\n",
        "# Configure Gemini\n",
        "GEMINI_API_KEY = \"AIzaSyBVESVD2g6qJvDiIcS5BkQahqwNHanKh0E\"\n",
        "API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key={GEMINI_API_KEY}\"\n",
        "\n",
        "# Store outputs here\n",
        "outputs = {}\n"
      ],
      "metadata": {
        "id": "aF-PyPfi5BN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Define the prompt options\n",
        "prompt_options = {\n",
        "    \"Activity 1 – Identify Common Policies\": (\n",
        "        \"Implement a Prompt Chain to identify common AI-generated policy recommendations for earthquake and wildfire preparedness in India, \"\n",
        "        \"using policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’). \"\n",
        "        \"Follow this step-by-step implementation process: Task Decomposition: Break down the task of identifying common policy recommendations into sub-tasks: \"\n",
        "        \"(a) Summarize earthquake policies, (b) Summarize wildfire policies, (c) Compare for overlaps, (d) Categorize commonalities, and (e) Refine for India’s context. \"\n",
        "        \"Use AI to brainstorm sub-task clarity, ensuring alignment with document policies like ‘Urban Evacuation Micro-Zoning’ (earthquake) and ‘Decentralized Alert System’ (wildfire). \"\n",
        "        \"Node Creation: Formulate natural language prompts for each sub-task: Node 1: ‘List earthquake policies from the document, such as “Multi-Tiered Alert Framework” and “CBEP,” noting prevention and response elements for urban/rural India.’ \"\n",
        "        \"Node 2: ‘List wildfire policies, like “VFFCs” and “Pine Needle Management,” detailing prevention and response for forest/rural areas.’ \"\n",
        "        \"Node 3: ‘Compare earthquake and wildfire policies to identify shared themes, like alert systems or community roles, referencing document rationales (e.g., “life-saving potential”).’ \"\n",
        "        \"Node 4: ‘Categorize common policies into themes (e.g., tech, governance), ensuring relevance to India’s disaster landscape.’ \"\n",
        "        \"Node 5: ‘Refine common policies, suggesting enhancements like offline alerts for rural gaps, based on document insights.’ \"\n",
        "        \"Node Connection: Validate logical connections between nodes to ensure flow: Node 1 and 2 outputs feed Node 3 for comparison, Node 3 informs Node 4’s categorization, and Node 4 guides Node 5’s refinement. \"\n",
        "        \"Use AI to check if connections align (e.g., does ‘UDNS’ link to ‘SMS Alerts’ logically?). Chain Refinement: Analyze initial outputs for completeness, identifying gaps (e.g., missing urban wildfire policies) or redundancies. \"\n",
        "        \"Use AI to suggest refinements, such as merging similar alert policies or adding urban buffer zones for wildfires, drawing on document best practices (e.g., Tokyo’s mapping, Indonesia’s Destana). \"\n",
        "        \"User  Feedback Integration: Structure a feedback mechanism to validate outputs, such as a checklist for policy relevance (e.g., ‘Does it address rural India?’). \"\n",
        "        \"Use AI to analyze feedback and adjust the chain, ensuring the final output is a table of 5–10 common policies (e.g., early warning, community training), with stakeholders, themes, and India-specific adaptations, incorporating document specifics like ‘EPI’ or ‘MGNREGA links.’\"\n",
        "    ),\n",
        "    \"Activity 2 – Evaluate Region-Specific vs. Generalizable Policies\": (\n",
        "        \"Visualize a team of three world-class disaster policy experts—a policy analyst, a field-level emergency responder, and a technology and infrastructure planner—collaboratively evaluating the adaptability of an AI-generated disaster response policy. \"\n",
        "        \"They will follow the Tree of Thoughts approach, where each expert shares their step-by-step reasoning, openly acknowledges uncertainties, corrects assumptions, and builds upon each other’s perspectives. \"\n",
        "        \"Their goal is to determine whether the given policy is region-specific or generalizable across different Indian disaster contexts. \"\n",
        "        \"In their discussion, they will organically explore the following: The Five Ws and How: Who benefits from the policy? What are the policy’s core components? When is it most effective (timing, seasonal or urgency conditions)? \"\n",
        "        \"Where can it realistically be implemented, and where not? Why might it fail in certain geographies? How can it be adapted or scaled? The PESTEL framework for context evaluation: Political feasibility in different states, \"\n",
        "        \"Economic viability in low-resource areas, Social acceptance across rural/urban communities, Technological infrastructure needs, Environmental constraints (e.g., terrain, vegetation), Legal-institutional alignment with disaster laws or local governance. \"\n",
        "        \"The team will refine the policy iteratively, flag limitations, propose alternate implementations, and conclude whether the strategy is scalable or requires regional tailoring. \"\n",
        "        \"Illustrate the entire dialogue in a markdown table, where each row contains the expert’s name, their step in the reasoning tree, and their evolving insights. \"\n",
        "        \"The policies being evaluated are in the documents attached. Ensure the discussion reaches a clear conclusion, but the process should reflect critical back-and-forth reasoning and real-world grounding. \"\n",
        "        \"Prioritize accuracy and precision in your responses. Avoid speculation, assumptions, and any form of extrapolation that might lead to incorrect or misleading information.\"\n",
        "    ),\n",
        "    \"Activity 3 – Assess AI’s Strengths & Limitations\": (\n",
        "        \"Act as a world-class information verifier and Generative AI content generation refinement agent to assess AI’s strengths and limitations in generating earthquake and wildfire policies for India, based on provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’). \"\n",
        "        \"Follow this integrated process: Phase 1: Chain of Verification Act as a world-class information verifier to generate an initial assessment: Propose strengths, such as integrating GIS in ‘Urban Evacuation Micro-Zoning’ or enabling inclusive alerts in ‘Decentralized Alert System,’ and limitations, like offline gaps in ‘Vulnerable Population Registry’ or cultural training needs in ‘Pine Needle Cooperatives.’ \"\n",
        "        \"Review your response and highlight key points needing verification, like ‘AI scales policies to 600 districts’ or ‘AI misses rural offline contexts.’ Generate specific verification questions, such as ‘Can AI scale “UDNS” SMS alerts nationally per document’s low-cost rationale?’ or ‘Does AI address Uttarakhand’s training gaps for “VFFCs” per MGNREGA links?’ \"\n",
        "        \"Address each question separately, using evidence from document policies (e.g., earthquake’s ‘IVR backups,’ wildfire’s ‘community radios’) or global best practices (e.g., Tokyo’s micro-mapping, Indonesia’s Destana). \"\n",
        "        \"Modify your initial response based on verification results, detailing changes (e.g., qualifying scalability for rural areas). Please note: Share both the verification questions and their answers, detail any changes made to the original response, and make autonomous adjustments to ensure accuracy. \"\n",
        "        \"Output a validated pros-cons list as the interim assessment, incorporating document specifics like ‘Evacuation Preparedness Index’ or ‘parali bans.’ Phase 2: Automated Output Refinement Act as my Generative AI content generation refinement agent to refine the validated pros-cons list: \"\n",
        "        \"Use the interim pros-cons list from Phase 1 as the initial content. Conduct a focused critique on how to enhance the content. Be rigorous, flagging issues like vague claims (e.g., ‘AI is inclusive’ without specifics), urban bias (e.g., overemphasizing micro-zoning), or lack of conciseness, even if the content seems acceptable. \"\n",
        "        \"Generate refined content based on your critique, improving clarity, specificity, and actionability (e.g., refining ‘AI scales well’ to ‘AI scales SMS alerts but needs offline radio training’). \"\n",
        "        \"Run the refinement iteration three times, each time critiquing and enhancing the previous output, ensuring alignment with document metrics (e.g., ‘low-cost,’ ‘life-saving potential’) and India’s urban-rural context. \"\n",
        "        \"Output the final refined pros-cons list after three iterations, detailing each critique and improvement made, ensuring the assessment is precise, evidence-based, and tailored to India’s disaster policy making context. Include a summary table with columns: Strength/Limitation, Description, Document Evidence, Refinement Notes.\"\n",
        "    ),\n",
        "    \"Activity 4 – Score & Rank Policies\": (\n",
        "        \"Step 1: Self-Consistency Sub-Prompt 1 - Urban Effectiveness Purpose: Generate initial scores for earthquake and wildfire policies focusing on urban effectiveness, prioritizing metrics like evacuation speed or fire suppression impact. \"\n",
        "        \"Prompt: Score AI-generated earthquake and wildfire policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’) for urban effectiveness, prioritizing metrics like evacuation speed, fire suppression impact, or lives saved. \"\n",
        "        \"Include policies such as ‘Urban Evacuation Micro-Zoning’ and ‘Unified Disaster Notification System’ (earthquake), and ‘Decentralized Alert System’ and ‘Parali Ban’ (wildfire). Assign numerical scores (1–10) for each policy based on their impact in urban India, referencing document specifics like ‘SMS alerts’ or ‘GIS dashboards.’ \"\n",
        "        \"Output a table with columns: Policy Name, Effectiveness Score, Rationale, ensuring alignment with India’s urban context. Step 2: Self-Consistency Sub-Prompt 2 - Rural Feasibility Purpose: Generate scores for earthquake and wildfire policies focusing on rural feasibility, emphasizing metrics like cost and infrastructure constraints. \"\n",
        "        \"Prompt: Score AI-generated earthquake and wildfire policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’) for rural feasibility, emphasizing metrics like cost, infrastructure availability, and community adoption in rural India. \"\n",
        "        \"Include policies such as ‘Community-Based Evacuation Planning’ and ‘Vulnerable Population Registry’ (earthquake), and ‘Village Fire Committees’ and ‘Pine Needle Management’ (wildfire). Assign numerical scores (1–10) for each policy based on their practicality in rural settings, referencing document specifics like ‘low-cost loudspeakers,’ ‘MGNREGA links,’ or ‘Gram Panchayat mapping.’ Output a table with columns: Policy Name, Feasibility Score, Rationale, ensuring alignment with India’s rural context. Step 3: Self-Consistency Sub-Prompt 3 - National Scalability Purpose: Generate scores for earthquake and wildfire policies focusing on national scalability, assessing their applicability across India’s diverse states. Prompt: Score AI-generated earthquake and wildfire policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’) for national scalability, assessing their applicability across India’s states, prioritizing metrics like stakeholder coordination, tech infrastructure, and regional adaptability. Include policies such as ‘Evacuation Preparedness Index’ and ‘Multi-Tiered Alert Framework’ (earthquake), and ‘Village Fire Committees’ and ‘Decentralized Alert System’ (wildfire). Assign numerical scores (1–10) for each policy based on their potential to scale nationwide, referencing document specifics like ‘Smart Cities funding,’ ‘SMS alerts,’ or ‘scalable nationwide’ claims. Output a table with columns: Policy Name, Scalability Score, Rationale, ensuring alignment with India’s diverse urban-rural and regional context. Step 4: Self-Consistency - Consistency Evaluation Purpose: Analyze the three sets of scores (urban effectiveness, rural feasibility, national scalability) for coherence, identifying consistent patterns and resolving outliers to prepare for verification. Prompt: Analyze the three sets of scores for AI-generated earthquake and wildfire policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’), previously generated for urban effectiveness, rural feasibility, and national scalability. Include policies like ‘Unified Disaster Notification System’ and ‘Evacuation Preparedness Index’ (earthquake), and ‘Decentralized Alert System’ and ‘Village Fire Committees’ (wildfire). Follow these steps: 1) Compare scores across the three dimensions to identify patterns (e.g., policies scoring consistently high, like ‘UDNS’ for effectiveness and scalability). 2) Flag outliers (e.g., a policy scoring high for urban effectiveness but low for rural feasibility, such as ‘Urban Evacuation Micro-Zoning’). 3) Propose adjustments for outliers, justifying with document specifics (e.g., ‘low-cost SMS’ for scalability, ‘rural tech gaps’ for feasibility). Output a table with columns: Policy Name, Urban Effectiveness Score, Rural Feasibility Score, Scalability Score, Consistency Notes, and Proposed Adjustments, ensuring alignment with India’s urban-rural and regional context. Step 5: Chain of Verification - Score Validation Purpose: Validate the adjusted scores from the consistency evaluation by generating questions, answering them with document evidence, and refining scores to ensure accuracy. Prompt: Act as a world-class information verifier to validate the adjusted scores for AI-generated earthquake and wildfire policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’), based on the consistency evaluation of urban effectiveness, rural feasibility, and national scalability. Include policies like ‘Unified Disaster Notification System’ and ‘Evacuation Preparedness Index’ (earthquake), and ‘Decentralized Alert System’ and ‘Village Fire Committees’ (wildfire). Follow this verification process: Review the adjusted scores and highlight key points needing verification, such as ‘“UDNS” scores 8/10 for scalability’ or ‘“VFFCs” scores 6/10 for feasibility.’ Generate specific verification questions, like ‘Does “UDNS” scalability align with the document’s claim of 600-district SMS coverage?’ or ‘Is “VFFCs” feasibility limited by rural training gaps per MGNREGA evidence?’ Answer each question separately, using evidence from document policies (e.g., earthquake’s ‘IVR backups,’ wildfire’s ‘community radios’) or global best practices (e.g., Indonesia’s Destana village planning, Tokyo’s micro-mapping). Modify scores based on verification results, detailing changes (e.g., ‘Lower “Pine Cooperatives” feasibility due to regional training limits’). Share both the verification questions and their answers, detail all changes made, and make autonomous adjustments for accuracy. Output a revised table with columns: Policy Name, Urban Effectiveness Score, Rural Feasibility Score, Scalability Score, Verification Notes, ensuring alignment with India’s disaster policymaking context. Step 6: Final Synthesis and Output Purpose: Synthesize the verified scores from the consistency evaluation and verification steps into a prioritized ranking of earthquake and wildfire policies, ensuring a reliable and evidence-based final output. Prompt: Synthesize the verified scores for AI-generated earthquake and wildfire policies from provided documents (‘Earthquake Final Policies.docx’ and ‘Policy Recommendations for Wildfires.docx’), based on the consistency evaluation and verification outputs for urban effectiveness, rural feasibility, and national scalability. Include policies like ‘Unified Disaster Notification System’ and ‘Evacuation Preparedness Index\"\n",
        "    )\n",
        "}\n",
        "\n",
        "prompt_dropdown = widgets.Dropdown(\n",
        "    options=prompt_options,\n",
        "    description='Select Prompt:',\n",
        "    layout=widgets.Layout(width='95%')\n",
        ")\n",
        "\n",
        "display(prompt_dropdown)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "49e1010f1b6346e6a1964bf23cfaa365",
            "cc8cb2aad908454481e787ccc5d0533e",
            "c4518718d51c42b891c784c1ec6a278d"
          ]
        },
        "id": "q7BfFD7W5Ilp",
        "outputId": "78fb0474-1a97-44b8-b421-78de652c0dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Select Prompt:', layout=Layout(width='95%'), options={'Activity 1 – Identify Common Poli…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "49e1010f1b6346e6a1964bf23cfaa365"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: Load and chunk documents\n",
        "def extract_text(path):\n",
        "    doc = Document(path)\n",
        "    return \"\\n\".join([p.text for p in doc.paragraphs if p.text.strip()])\n",
        "\n",
        "wildfire_text = extract_text(\"Policy Recommendations for Wildfires.docx\")\n",
        "earthquake_text = extract_text(\"Earthquake Final Policies.docx\")\n",
        "combined_text = wildfire_text + \"\\n\" + earthquake_text\n",
        "\n",
        "def chunk_text(text, chunk_size=1000, overlap=200):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunks.append(\" \".join(words[i:i + chunk_size]))\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(combined_text)\n"
      ],
      "metadata": {
        "id": "E-JsJKfh5N04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: Retrieve context using TF-IDF\n",
        "def retrieve_context(question, k=3):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    chunk_vectors = vectorizer.fit_transform(chunks)\n",
        "    query_vec = vectorizer.transform([question])\n",
        "    similarities = cosine_similarity(query_vec, chunk_vectors).flatten()\n",
        "    top_indices = similarities.argsort()[-k:][::-1]\n",
        "    return \"\\n\\n\".join([chunks[i] for i in top_indices])\n"
      ],
      "metadata": {
        "id": "i7zFFcZ_8Ns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Ask Gemini with extended token support\n",
        "def ask_gemini(prompt_text, context):\n",
        "    full_prompt = f\"\"\"Use the following context to perform the analysis task below:\n",
        "\n",
        "{prompt_text}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "    payload = {\n",
        "        \"contents\": [{\n",
        "            \"parts\": [{\"text\": full_prompt}]\n",
        "        }],\n",
        "        \"generationConfig\": {\n",
        "            \"maxOutputTokens\": 32768\n",
        "        }\n",
        "    }\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    response = requests.post(API_URL, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        return result[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "    else:\n",
        "        return f\"Error {response.status_code}: {response.text}\"\n"
      ],
      "metadata": {
        "id": "ANJxHcwc8R7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Run selected prompt\n",
        "selected_prompt = prompt_dropdown.label\n",
        "context = retrieve_context(prompt_dropdown.value)\n",
        "response_text = ask_gemini(prompt_dropdown.value, context)\n",
        "\n",
        "outputs[selected_prompt] = response_text\n",
        "\n",
        "print(f\"\\n=== Gemini Output for {selected_prompt} ===\\n\")\n",
        "print(response_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-3b42B08ZU6",
        "outputId": "00091999-c34a-477a-e2cd-8536477f3794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Gemini Output for Activity 1 – Identify Common Policies ===\n",
            "\n",
            "## Analysis of Prompt Chain for AI-Generated Policy Recommendations\n",
            "\n",
            "This analysis evaluates the provided prompt chain for identifying common AI-generated policy recommendations for earthquake and wildfire preparedness in India.\n",
            "\n",
            "**Strengths:**\n",
            "\n",
            "* **Clear Task Decomposition:** Breaking the task into sub-tasks (summarization, comparison, categorization, refinement, and contextualization) provides a structured approach.\n",
            "* **Specific Node Prompts:**  The prompts are detailed and reference specific policies from the documents (e.g., \"Urban Evacuation Micro-Zoning,\" \"Decentralized Alert System,\" \"VFFCs,\" \"Pine Needle Management\"), guiding the AI towards relevant information extraction.\n",
            "* **Emphasis on Indian Context:** The chain repeatedly emphasizes the need to tailor recommendations to the Indian context, considering urban/rural divides, language diversity, and existing programs like MGNREGA.\n",
            "* **Integration of Best Practices:** The chain encourages the AI to draw upon global best practices mentioned in the documents (e.g., Tokyo's micro-mapping, Indonesia's Destana) for enhanced recommendations.\n",
            "* **Multi-tiered Communication:** The chain addresses the need for diverse communication strategies, including offline options for areas with limited connectivity, reflecting the realities of India's digital landscape.\n",
            "* **User Feedback Integration:**  Incorporating a feedback mechanism and using AI to analyze it ensures the final output aligns with user needs and policy relevance.\n",
            "* **Specific Output Format:**  The desired output format (a table with stakeholders, themes, and India-specific adaptations) promotes clarity and actionable recommendations.\n",
            "\n",
            "\n",
            "**Weaknesses and Refinement Suggestions:**\n",
            "\n",
            "* **Missing Urban Wildfire Policies:** The current focus heavily leans towards forest wildfires.  Node 2 should be expanded to explicitly include urban wildfire risks and potential policies (e.g., building codes, green buffer zones, evacuation plans for urban-wildland interface areas).  The prompt could be revised to: \"List wildfire policies, like “VFFCs” and “Pine Needle Management,” detailing prevention and response for forest/rural areas *and urban-wildland interface zones. Consider policies relevant to building materials, urban green spaces, and community preparedness in densely populated areas.*\"\n",
            "* **Limited Cross-Hazard Policy Exploration:** While the chain identifies common themes, it could further explore policies that address both earthquake and wildfire preparedness simultaneously (e.g., multi-hazard early warning systems, community disaster resilience training, strengthening local governance for all disaster types). Node 3 should be enhanced to prompt for this explicitly: \"Compare earthquake and wildfire policies to identify shared themes, like alert systems or community roles, referencing document rationales (e.g., “life-saving potential”). *Also, explore policies that could address both hazards simultaneously, considering resource optimization and community capacity building.*\"\n",
            "* **Node Connection Validation:** While mentioned, the chain could be more explicit about *how* AI will be used to validate logical connections.  For instance, using semantic similarity analysis or knowledge graphs to verify the link between ‘UDNS’ and ‘SMS Alerts’. The prompt for Node Connection should be revised to: \"Validate logical connections between nodes to ensure flow: Node 1 and 2 outputs feed Node 3 for comparison, Node 3 informs Node 4’s categorization, and Node 4 guides Node 5’s refinement. *Use AI-powered semantic similarity analysis or knowledge graphs to verify the logical connection between concepts.  For example, analyze the relationship between ‘UDNS’ and ‘SMS Alerts’ to ensure a coherent flow of information.*\"\n",
            "\n",
            "* **Feedback Mechanism Detail:**  The feedback mechanism needs more structure.  A specific checklist of criteria (e.g., cost-effectiveness, scalability, political feasibility, community acceptance) would make feedback more targeted and actionable.\n",
            "\n",
            "**Example of a Refined Node and Connection:**\n",
            "\n",
            "**Refined Node 2:** \"List wildfire policies, detailing prevention and response for forest/rural areas and urban areas. Include policies related to building codes, green buffer zones, community preparedness, flammable material management, public awareness campaigns, and integration with urban planning.\"\n",
            "\n",
            "**Refined Connection between Nodes 2 and 3:** \"Verify the logical connection between wildfire policies from Node 2 and the comparison process in Node 3. For example, ensure that policies addressing urban wildfires are also considered during the comparison with earthquake policies. Use AI-powered concept mapping to visualize the relationships between different policy areas and ensure comprehensive coverage.\"\n",
            "\n",
            "\n",
            "By addressing these weaknesses, the prompt chain can be strengthened to generate more comprehensive and contextually relevant policy recommendations for disaster preparedness in India.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}